{
  "name": "Som mnist",
  "tagline": "Repository for SOM implementation on MNIST",
  "body": "### Self Organizing maps\r\nThe topic of this page is Self Organizing maps (SOM). A SOM results in a new representation of your high-dimensional data. This new representation is usually 2D or 3D, so it can be interpreted by humans. The inspiration for SOM comes from neuroscience. The structuring that neurons use to represent information is the same structuring that SOM is trained for.\r\n\r\n### Representation\r\nNeurons represent information with the following maxim \"local neurons, represent similar information\". From this statement, the algorithm is derived. For simplicity, we put our neurons in a square lattice:\r\n\r\n![image of lattice](https://github.com/RobRomijnders/SOM_blog/blob/master/lattice.png?raw=true)\r\n*Image from Haykin09*\r\n\r\nNow, in plain English words, we expect the following from our neurons:\r\n* Neighboring neurons represent similar parts of the input space. The concept \"neighbor\" is defined in the lattice.\r\n* Contrary to the first point, we want the neurons to represent the complete input space. Therefore, we allow neurons during the training process to differentiate with respect to their neighbors.\r\n\r\nTo achieve this dichotomy between neurons representing similar parts of the input space, but also represent the complete input space, we train the SOM with these two settings:\r\n* During the training, we narrow the neighborhood function. That is, in the start of the training, we define neighboring neurons in a broad manner. Over time, the neighboring neurons will influence eachother less.\r\n* To allow the differentiation, we decrease the adaptation of the neurons in the input space. Initially, the neurons can move very quickly in the input space. Over time, the so-called learning rate decreases. With smaller learning rates, the neurons move less abruptly in the input space.\r\n\r\n### Running an SOM on MNIST\r\nEventually, we work toward representations like this:\r\n![image of SOM in MNIST ](https://github.com/RobRomijnders/SOM_MNIST/blob/master/vis_nn.png?raw=true)\r\n*SOM on MNIST*\r\nIn this image, we deploy a ten by ten lattice. The dataset is MNIST, where numbers 1 and 8 are left out.\r\n\r\n### General outline of the training procedure\r\nDuring the training procedure, we want neighboring neurons to specialize to some part of the input space. To this end, we pick random samples from the dataset and allign neurons with them. This allignment holds for the neuron that is already closest to the incoming sample. And its neighbors of course.\r\nSo the outline will be:\r\n* Pick a random sample from the dataset\r\n* Determine the closest neuron\r\n* Move the winning neuron towards the incoming sample. Also move the neighbors in the direction of this sample, but to a lesser amount. This amount is determined by the neighborhood function in the lattice. \r\n  * The moves in the input space are parameterized by a learning rate.\r\n  * The neighborhood function in the lattice is parameterized by a variance.\r\n* For the next step:\r\n  * Update the learning rate\r\n  * Update the variance\r\n* Repeat until convergence\r\n\r\n### Tips for debugging\r\n* The mechanisms of SOM take place on two spaces. One is your input space, which can be high dimensional. Second is your lattice, which is usually 2D or 3D. These concepts are tied together by the neurons. Each neuron has a location in the lattice, which remains fixed throughout training. The neuron also has a location in the input space. The latter is sometimes referred to as his weights. \r\n* Your first and foremost debugging step is to plot your learning rates and variances. I've been helping peer students that were debugging for hours, only to discover their learning rate decay was faulty.\r\n* Plot a few of your weights as a monitor. Your neurons are represented by numbers in some matrix you defined. By plotting these numbers, you can develop a feel for the training procedure. I've included my example below.\r\n![Monitor for training SOM](https://github.com/RobRomijnders/SOM_MNIST/blob/master/monitor.png?raw=true)\r\n\r\nThe code is on my GitHub in the repository SOM_MNIST. It contains many comment statements so that you can adapt it to your own liking or use it to compare your implementation of SOM.\r\n\r\nAs always, I am curious to any comments and questions. Reach me at romijndersrob@gmail.com ",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}